{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPs6P0pWfkQy80p0BY3zEO7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BennoKrojer/ML2/blob/main/Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTqmXeilnZp2"
      },
      "source": [
        "# COMP 551 - Mini Project 2\n",
        "\n",
        "Starting point: [sklearn text data tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)  \n",
        "\n",
        "Recommended Base Model: [sklearn's Logistic Regression package](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ahukbl7nUC4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO0fekBkqOrl"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykr4JRvoqOVN",
        "outputId": "b1815b3a-5243-4930-fe50-f25a9518cb2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/BennoKrojer/ML2/main/fake_news_data/fake_news_train.csv\n",
        "!wget https://raw.githubusercontent.com/BennoKrojer/ML2/main/fake_news_data/fake_news_val.csv\n",
        "!wget https://raw.githubusercontent.com/BennoKrojer/ML2/main/fake_news_data/fake_news_test.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-19 00:08:38--  https://raw.githubusercontent.com/BennoKrojer/ML2/main/fake_news_data/fake_news_train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63830815 (61M) [text/plain]\n",
            "Saving to: ‘fake_news_train.csv’\n",
            "\n",
            "fake_news_train.csv 100%[===================>]  60.87M   150MB/s    in 0.4s    \n",
            "\n",
            "2021-10-19 00:08:43 (150 MB/s) - ‘fake_news_train.csv’ saved [63830815/63830815]\n",
            "\n",
            "--2021-10-19 00:08:43--  https://raw.githubusercontent.com/BennoKrojer/ML2/main/fake_news_data/fake_news_val.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6533293 (6.2M) [text/plain]\n",
            "Saving to: ‘fake_news_val.csv’\n",
            "\n",
            "fake_news_val.csv   100%[===================>]   6.23M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-10-19 00:08:43 (58.8 MB/s) - ‘fake_news_val.csv’ saved [6533293/6533293]\n",
            "\n",
            "--2021-10-19 00:08:44--  https://raw.githubusercontent.com/BennoKrojer/ML2/main/fake_news_data/fake_news_test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9484222 (9.0M) [text/plain]\n",
            "Saving to: ‘fake_news_test.csv’\n",
            "\n",
            "fake_news_test.csv  100%[===================>]   9.04M  54.2MB/s    in 0.2s    \n",
            "\n",
            "2021-10-19 00:08:45 (54.2 MB/s) - ‘fake_news_test.csv’ saved [9484222/9484222]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4EotE27rZKj",
        "outputId": "57de4d54-1395-41e5-b658-1a18e6c4505e"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fake_news_test.csv  fake_news_train.csv  fake_news_val.csv  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ctx0Og8rPKS",
        "outputId": "2ab7198f-aef7-4ab1-d379-1a2e37927a7b"
      },
      "source": [
        "raw_data_train = pd.read_csv(\"fake_news_train.csv\")\n",
        "raw_data_val   = pd.read_csv(\"fake_news_val.csv\")\n",
        "raw_data_test  = pd.read_csv(\"fake_news_test.csv\")\n",
        "\n",
        "raw_data_train.head\n",
        "raw_data_train.columns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'label'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICbXsDsDqRFA"
      },
      "source": [
        "### Data Pre-Processing\n",
        "To preform machine learning on text, need to first extract features for the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu7tBzUNyDC3",
        "outputId": "807ed43e-cdec-4719-d788-9a4677cd146a"
      },
      "source": [
        "# Seperate raw data into text and label\n",
        "raw_data_train_text = raw_data_train[\"text\"]\n",
        "raw_data_train_label = raw_data_train[\"label\"]\n",
        "\n",
        "# Create numpy array to store target labels\n",
        "Y_train = np.asarray(raw_data_train_label)\n",
        "Y_train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_pGa57e70GT",
        "outputId": "3ef4ea7f-9282-45a6-ee1f-8babae593a45"
      },
      "source": [
        "# Seperate raw data for validation set\n",
        "raw_data_val_text = raw_data_val[\"text\"]\n",
        "raw_data_val_label = raw_data_val[\"label\"]\n",
        "Y_val = np.asarray(raw_data_val_label)\n",
        "Y_val"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOmGN5-v71-L",
        "outputId": "6b921ab7-dc87-48df-abb0-75086be594bb"
      },
      "source": [
        "# Seperate raw data for test set\n",
        "raw_data_test_text = raw_data_test[\"text\"]\n",
        "raw_data_test_label = raw_data_test[\"label\"]\n",
        "Y_test = np.asarray(raw_data_test_label)\n",
        "Y_test"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfK94Xnxsmlw"
      },
      "source": [
        "#### Bag of Words (Vectorizing)\n",
        "For the bag of words method, each word in the training data set is given an integer ID, then for each data sample, count the number of occurrences of each word, and store the count of each word as a feature for the training sample.  \n",
        "\n",
        "Ex:  \n",
        "`Data sample 'i' = \"The quick brown fox jumped over the brown dog\"`  \n",
        "`if id for word \"brown\" = 3`  \n",
        "`store X[i, 3] = 2 (word count)`    \n",
        "\n",
        "This method implies that the number of features = number of unique words in all training samples. Number features is typically > 100,000.  \n",
        "\n",
        "Thus, if every sample had every word, then the size of the matrix would be 100,000 x 100,000 x 4 bytes, which is not very practical. Luckily, most features will be zero for most samples, as most features only contain a small subset of the total set of words in the data set. For this reason, we usually say that the bag of words features array are *high-dimensional sparse data sets*.  \n",
        "\n",
        "Will use scipy.sparse matrix to store data set features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwiZn_7ErrzB",
        "outputId": "e6df4a9d-0961-4b2d-e78b-5c1c8a237ff9"
      },
      "source": [
        "# Use sklearn.feature_extraction.text.CountVectorizer\n",
        "# To build a dictionary of features, and transform data samples into feature vectors\n",
        "count_vect = CountVectorizer()\n",
        "\n",
        "# training feature vectors\n",
        "X_train_counts = count_vect.fit_transform(raw_data_train_text)\n",
        "X_train_counts.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 145402)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f63qwg4yoZQ",
        "outputId": "6a78c072-777e-4167-9ffd-ccd204c50d6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Peak at output of count vectorizer\n",
        "count_vect.vocabulary_.get(u'algorithm')\n",
        "#count_vect.vocabulary_.get(u' ')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10739"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgiRIKFYzQLg"
      },
      "source": [
        "#### Occurences -> Frequencies (Transforming)\n",
        "Occurence count is a good start, but is skewed towards longer documents. Longer text will on average have more occurences than shorter text. To compensate for this, we can look at term frequencies which is the number of occurences of a word in some text, divided by the total number of words in that text.  \n",
        "\n",
        "#### Downscale Word Weights\n",
        "Another pre-processing technique for text is to give higher weights to rarer words, and lower weights to words that appear frequently in the overall set of text. This is down by downscaling the weights of words that appear frequently in all texts.  \n",
        "\n",
        "These two approaches can be combined into something called tf-idf, or Term Frequency times Inverse Document Frequency. Can be computed using sklearn->TfidfTransformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNVBdsHYzosi",
        "outputId": "92762677-3131-49b7-cc80-5afa6acceaca"
      },
      "source": [
        "tfidf_transformer = TfidfTransformer()\n",
        "# fit_transform combines fit and transform into one step\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_train_tfidf.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 145402)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9t-EmYM5mc7"
      },
      "source": [
        "#### Scaling Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLYAELU0rsHs"
      },
      "source": [
        "### Model Training\n",
        "Linear model - Logistic Regression using sklearn.linear_model.LogisticRegression  \n",
        "\n",
        "Start with default parameters for Logistic Regression classifier:  \n",
        "- penalty  - norm of the penalty (default = 'l2')  \n",
        "- tol      - tolerance for stopping criteria (default = 1e-4)  \n",
        "- max_iter - Maximum number of iterations taken for the solvers to converge (default = 100) \n",
        "\n",
        "can also specify # cpus to use\n",
        "n_jobs = default = None: means 1, use -1 to specify all processors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRkRtxBq7SCb"
      },
      "source": [
        "#### Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-VqA9eCrsW2",
        "outputId": "7e993055-6c1c-4f33-86bb-613247314a08"
      },
      "source": [
        "# is model same as classifier?\n",
        "# clf = classifier\n",
        "clf = LogisticRegression(penalty='l2', tol=1e-4, max_iter=1000)\n",
        "\n",
        "# fit classifier\n",
        "clf.fit(X_train_tfidf, Y_train)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQnrepY87The"
      },
      "source": [
        "#### Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwkimEED7Upb",
        "outputId": "7945a75f-09ef-4f12-94aa-77c336fc3b0b"
      },
      "source": [
        "# Pre process validation data in same way as train data\n",
        "# But only transform data, as count_vectorizer and tfidf_transformer have already been\n",
        "# fit to training data (and don't want to fit to validation or test data)\n",
        "\n",
        "X_val_counts = count_vect.transform(raw_data_val_text)\n",
        "print(\"X_val_counts.shape: \", X_val_counts.shape)\n",
        "X_val_tfidf = tfidf_transformer.transform(X_val_counts)\n",
        "print(\"X_val_tfidf.shape: \", X_val_tfidf.shape)\n",
        "\n",
        "val_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "print(\"val_pred.shape: \", val_pred.shape)\n",
        "print(\"Y_val.shape: \", Y_val.shape)\n",
        "\n",
        "# Compare predicitions to labels\n",
        "np.mean(val_pred == Y_val)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_val_counts.shape:  (2000, 145402)\n",
            "X_val_tfidf.shape:  (2000, 145402)\n",
            "val_pred.shape:  (2000,)\n",
            "Y_val.shape:  (2000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.731"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k__mlSeV9pWy"
      },
      "source": [
        "### Pipeline\n",
        "In order to make our sequence of pre-processing and classifying easier, sklearn allows us to create a pipeline to apply the operations to a set of data: vectorizer->transformer->classifier.  \n",
        "- Vectorizer (\"vect\") = Bag of Words, transform text to counts of words in text\n",
        "- Transformer (\"tfidf\") = TF-IDF transform\n",
        "- Classifier (\"lr-clf\") = Logistic Regression classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiHfFd-i-EOY"
      },
      "source": [
        "text_pipeline = Pipeline([\n",
        "    (\"vect\", CountVectorizer()),\n",
        "    (\"tfidf\", TfidfTransformer()),\n",
        "    (\"lr-clf\", LogisticRegression(max_iter=500)),\n",
        "])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAtH1Sfy-sha",
        "outputId": "48b3bcab-f560-44f1-de3a-d375fb6ee54a"
      },
      "source": [
        "# Can now train using a single command / function\n",
        "text_pipeline.fit(raw_data_train_text, Y_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('lr-clf',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=500,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maWUNYZ8_vaT",
        "outputId": "96e5dbf3-5538-4ea9-ea85-e7238ff7556b"
      },
      "source": [
        "# Train pipeline using train set\n",
        "text_pipeline.score(raw_data_train_text, Y_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85525"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kaAuBPsAQzm",
        "outputId": "25d715b2-6178-4850-9250-07338a7333cf"
      },
      "source": [
        "# Predict labels for train set using pipeline\n",
        "Y_pred = text_pipeline.predict(raw_data_train_text)\n",
        "#print(\"Y_pred.shape: \", Y_pred.shape, \", Y_train.shape: \", Y_train.shape)\n",
        "acc = np.mean(Y_pred == Y_train)\n",
        "print(acc)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.85525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WppegNW5_Vu6",
        "outputId": "99e17c6f-1637-4f71-df79-2cb711a242da"
      },
      "source": [
        "# Validate using pipeline\n",
        "Y_pred = text_pipeline.predict(raw_data_val_text)\n",
        "#print(\"Y_pred.shape: \", Y_pred.shape, \", Y_val.shape: \", Y_val.shape)\n",
        "acc = np.mean(Y_pred == Y_val)\n",
        "print(acc)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A1umbRo3dBC"
      },
      "source": [
        "### Parameter Tuning\n",
        "\n",
        "First using grid search and then random search or other methods..\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        ">>> parameters = {\n",
        "...     'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "...     'tfidf__use_idf': (True, False),\n",
        "...     'clf__alpha': (1e-2, 1e-3),\n",
        "... }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roI7nYTt3n37"
      },
      "source": [
        "parameters = {\n",
        "    'vect_ngram_range': [(1,1), (1,2)],\n",
        "    'tfidf_use_idf': (True, False),\n",
        "    'lr_clf_penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'lr_clf_tol': ,\n",
        "    'lr_clr_max_iter': ,\n",
        "    \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px2g5iI0_EVF"
      },
      "source": [
        "### Model Evalutation (testing)\n",
        "Now test performance of model using testing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmRaZarKB3W2"
      },
      "source": [
        "#### Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keQx08nQBhus",
        "outputId": "38ea2749-9bd6-4126-9e4e-ce97b3d66345"
      },
      "source": [
        "# Test model acc with test data\n",
        "Y_pred = text_pipeline.predict(raw_data_test_text)\n",
        "acc = np.mean(Y_pred == Y_test)\n",
        "print(acc)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK0eFdQRB5J-"
      },
      "source": [
        "#### Model Metrics\n",
        "\n",
        "**double check this** might be backwards  \n",
        "Confusion Matrix: Positive = fake news, Negative = not fake news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zWMHpMcB0gG",
        "outputId": "21881080-cca0-4efc-a037-0161f463d329"
      },
      "source": [
        "# Confusion matrix for binary classification: \n",
        "metrics.confusion_matrix(raw_data_test_label, Y_pred)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 554,  676],\n",
              "       [ 194, 1576]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}