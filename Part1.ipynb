{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BennoKrojer/ML2/blob/main/Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWSYsw9UmKGh"
      },
      "source": [
        "# COMP 551 - Mini Project 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_FgLmtnt-xg"
      },
      "source": [
        "from numpy import genfromtxt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isoGhDjNMvxu"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/BennoKrojer/ML2/main/diabetes_data/diabetes_train.csv\n",
        "!wget https://raw.githubusercontent.com/BennoKrojer/ML2/main/diabetes_data/diabetes_test.csv\n",
        "!wget https://raw.githubusercontent.com/BennoKrojer/ML2/main/diabetes_data/diabetes_val.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBfyt74kqH9H",
        "outputId": "7cac6879-0d6f-4c46-8b4d-dbfbddfe6cf8"
      },
      "source": [
        "train = genfromtxt('diabetes_train.csv', delimiter=',', skip_header=1)\n",
        "test = genfromtxt('diabetes_test.csv', delimiter=',', skip_header=1)\n",
        "val = genfromtxt('diabetes_val.csv', delimiter=',', skip_header=1)\n",
        "train_x, train_y = train[:,:-1], train[:,-1]\n",
        "test_x, test_y = test[:,:-1], test[:,-1]\n",
        "val_x, val_y = val[:,:-1], val[:,-1]\n",
        "print(train_x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(600, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZYRn88auNYT"
      },
      "source": [
        "## Logistic Regression: Fully Batched"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X756pGNBlYyH"
      },
      "source": [
        "logistic = lambda z: 1./ (1 + np.exp(-z))       #logistic function\n",
        "\n",
        "def gradient(self, x, y):\n",
        "    N,D = x.shape\n",
        "    yh = logistic(np.dot(x, self.w))    # predictions  size N\n",
        "    grad = np.dot(x.T, yh - y)/N        # divide by N because cost is mean over N points\n",
        "    return grad                         # size D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv3xohIVuPz6"
      },
      "source": [
        "class LogisticRegression:\n",
        "\n",
        "    def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, max_iters=1e5, verbose=False, eval_steps=100):\n",
        "        self.add_bias = add_bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon  # to get the tolerance for the norm of gradients\n",
        "        self.max_iters = max_iters  # maximum number of iteration of gradient descent\n",
        "        self.verbose = verbose\n",
        "        self.eval_steps = eval_steps\n",
        "\n",
        "    def fit(self, x, y, val_x, val_y):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        if self.add_bias:\n",
        "            N = x.shape[0]\n",
        "            x = np.column_stack([x, np.ones(N)])\n",
        "        N, D = x.shape\n",
        "        self.w = np.zeros(D)\n",
        "        g = np.inf\n",
        "        t = 0\n",
        "        # the code snippet below is for gradient descent\n",
        "        steps = []\n",
        "        train_accs = []\n",
        "        val_accs = []\n",
        "        best_val_acc = 0\n",
        "        best_step = 0\n",
        "        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:\n",
        "            g = self.gradient(x, y)\n",
        "            self.w = self.w - self.learning_rate * g\n",
        "            t += 1\n",
        "            if t % self.eval_steps == 0:\n",
        "                yh = self.predict(x)\n",
        "                yh = yh > 0.5\n",
        "                preds = y == yh\n",
        "                train_acc = sum(preds) / len(preds)\n",
        "\n",
        "                val_yh = self.predict(val_x, add_bias=True)\n",
        "                val_yh = val_yh > 0.5\n",
        "                val_preds = val_y == val_yh\n",
        "                val_acc = sum(val_preds) / len(val_preds)\n",
        "\n",
        "                steps.append(t)\n",
        "                train_accs.append(train_acc)\n",
        "                val_accs.append(val_acc)\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    best_step = t\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f'terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g)}')\n",
        "            print(f'the weight found: {self.w}')\n",
        "        return self, steps, train_accs, val_accs, best_val_acc, best_step\n",
        "\n",
        "    def predict(self, x, add_bias=False):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        Nt = x.shape[0]\n",
        "        if add_bias:\n",
        "            x = np.column_stack([x, np.ones(Nt)])\n",
        "        yh = logistic(np.dot(x, self.w))  # predict output\n",
        "        return yh\n",
        "\n",
        "\n",
        "\n",
        "LogisticRegression.gradient = gradient             #initialize the gradient method of the LogisticRegression class with gradient function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Jje4DBpzwxi2",
        "outputId": "ba6b07bc-2167-42df-fa34-e92f4ccb47c2"
      },
      "source": [
        "for lr in [0.0004, 0.0002, 0.0001, 0.00005]:\n",
        "    logres = LogisticRegression(max_iters=2e6, learning_rate=lr)\n",
        "    model, steps, train_accs, val_accs, best_val_acc, best_step = logres.fit(train_x, train_y, val_x, val_y)\n",
        "    plt.plot(steps, train_accs, 'r', label='training accuracy')\n",
        "    plt.plot(steps, val_accs, 'b', label='validation accuracy')\n",
        "    plt.xlabel('iterations')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(loc='lower right', shadow=True, fontsize='medium')\n",
        "    plt.show()\n",
        "    print(lr)\n",
        "    print(best_val_acc)\n",
        "    print(best_step)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0002\n",
            "0.77\n",
            "709300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aCDx2jtiFgB"
      },
      "source": [
        "## Mini-Batch Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whHwKjokw1Eg"
      },
      "source": [
        "class MiniBatchLogisticRegression:\n",
        "\n",
        "    def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, batch_size=8, max_iters=1e5, verbose=False,\n",
        "                 eval_steps=100, shuffle=False):\n",
        "        self.add_bias = add_bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon  # to get the tolerance for the norm of gradients\n",
        "        self.max_iters = max_iters  # maximum number of iteration of gradient descent\n",
        "        self.verbose = verbose\n",
        "        self.eval_steps = eval_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def fit(self, x, y, val_x, val_y):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        if self.add_bias:\n",
        "            N = x.shape[0]\n",
        "            x = np.column_stack([x, np.ones(N)])\n",
        "        N, D = x.shape\n",
        "        self.w = np.zeros(D)\n",
        "        g = np.inf\n",
        "        t = 0\n",
        "        # the code snippet below is for gradient descent\n",
        "        steps = []\n",
        "        train_accs = []\n",
        "        val_accs = []\n",
        "        best_val_acc = 0\n",
        "        best_step = 0\n",
        "        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:\n",
        "            if self.shuffle:\n",
        "                p = np.random.permutation(int(y.shape[0]))\n",
        "                x = x[p]\n",
        "                y = y[p]\n",
        "            for i in range(0, int(y.shape[0]), self.batch_size):\n",
        "                batch_x = x[i:i+self.batch_size]\n",
        "                batch_y = y[i:i+self.batch_size]\n",
        "                g = self.gradient(batch_x, batch_y)\n",
        "                self.w = self.w - self.learning_rate * g\n",
        "            t += 1\n",
        "            if t % self.eval_steps == 0:\n",
        "                print(t)\n",
        "                yh = self.predict(x)\n",
        "                yh = yh > 0.5\n",
        "                preds = y == yh\n",
        "                train_acc = sum(preds) / len(preds)\n",
        "\n",
        "                val_yh = self.predict(val_x, add_bias=True)\n",
        "                val_yh = val_yh > 0.5\n",
        "                val_preds = val_y == val_yh\n",
        "                val_acc = sum(val_preds) / len(val_preds)\n",
        "\n",
        "                steps.append(t)\n",
        "                train_accs.append(train_acc)\n",
        "                val_accs.append(val_acc)\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    best_step = t\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f'terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g)}')\n",
        "            print(f'the weight found: {self.w}')\n",
        "        return self, steps, train_accs, val_accs, best_val_acc, best_step\n",
        "\n",
        "    def predict(self, x, add_bias=False):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        Nt = x.shape[0]\n",
        "        if add_bias:\n",
        "            x = np.column_stack([x, np.ones(Nt)])\n",
        "        yh = logistic(np.dot(x, self.w))  # predict output\n",
        "        return yh\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFrK85MxiEKi"
      },
      "source": [
        "MiniBatchLogisticRegression.gradient = gradient  # initialize the gradient method of the LogisticRegression class with gradient function\n",
        "\n",
        "taccs = {}\n",
        "vaccs = {}\n",
        "for size in [8, 16, 32, 64, 600]:\n",
        "    logres = MiniBatchLogisticRegression(max_iters=1e6, learning_rate=0.0002, batch_size=size)\n",
        "    model, steps, train_accs, val_accs, best_val_acc, best_step = logres.fit(train_x, train_y, val_x, val_y)\n",
        "    taccs[size] = train_accs\n",
        "    vaccs[size] = val_accs\n",
        "\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('accuracy')\n",
        "for size, vals in taccs.items():\n",
        "    plt.plot(steps, vals, label=f'batch size {size}')\n",
        "    plt.legend(loc='lower right', shadow=True, fontsize='medium')\n",
        "    plt.show()\n",
        "\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('accuracy')\n",
        "for size, vals in vaccs.items():\n",
        "    plt.plot(steps, vals, label=f'batch size {size}')\n",
        "    plt.legend(loc='lower right', shadow=True, fontsize='medium')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6I-LmN3vCUu"
      },
      "source": [
        "## Momentum Fully Batched Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJn4zy9d0JwZ"
      },
      "source": [
        "def gradient(self, x, y):\n",
        "    N, D = x.shape\n",
        "    yh = logistic(np.dot(x, self.w))  # predictions  size N\n",
        "    grad = np.dot(x.T, yh - y) / N  # divide by N because cost is mean over N points\n",
        "    J = np.mean(-y * np.log(yh) - (1-y) * np.log(1-yh))\n",
        "    return grad, J  # size D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lzett9OvFt0"
      },
      "source": [
        "class LogisticRegression:\n",
        "\n",
        "    def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, max_iters=1e5, verbose=False, eval_steps=100, momentum_beta=0.9):\n",
        "        self.add_bias = add_bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon  # to get the tolerance for the norm of gradients\n",
        "        self.max_iters = max_iters  # maximum number of iteration of gradient descent\n",
        "        self.verbose = verbose\n",
        "        self.eval_steps = eval_steps\n",
        "        self.momentum_beta = momentum_beta\n",
        "\n",
        "    def fit(self, x, y, val_x, val_y):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        if self.add_bias:\n",
        "            N = x.shape[0]\n",
        "            x = np.column_stack([x, np.ones(N)])\n",
        "        N, D = x.shape\n",
        "        self.w = np.zeros(D)\n",
        "        g = np.inf\n",
        "        t = 0\n",
        "        # the code snippet below is for gradient descent\n",
        "        steps = []\n",
        "        train_accs = []\n",
        "        val_accs = []\n",
        "        losses = []\n",
        "        best_val_acc = 0\n",
        "        best_step = 0\n",
        "        prev = np.zeros(D)\n",
        "        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:\n",
        "            g, loss = self.gradient(x, y)\n",
        "            g = self.momentum_beta * prev + (1-self.momentum_beta) * g\n",
        "            prev = g\n",
        "            self.w = self.w - self.learning_rate * g\n",
        "            t += 1\n",
        "            if t % self.eval_steps == 0:\n",
        "                yh = self.predict(x)\n",
        "                yh = yh > 0.5\n",
        "                preds = y == yh\n",
        "                train_acc = sum(preds) / len(preds)\n",
        "\n",
        "                val_yh = self.predict(val_x, add_bias=True)\n",
        "                val_yh = val_yh > 0.5\n",
        "                val_preds = val_y == val_yh\n",
        "                val_acc = sum(val_preds) / len(val_preds)\n",
        "\n",
        "                steps.append(t)\n",
        "                train_accs.append(train_acc)\n",
        "                val_accs.append(val_acc)\n",
        "                losses.append(loss)\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    best_step = t\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f'terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g)}')\n",
        "            print(f'the weight found: {self.w}')\n",
        "        return self, steps, train_accs, val_accs, best_val_acc, best_step, losses\n",
        "\n",
        "    def predict(self, x, add_bias=False):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        Nt = x.shape[0]\n",
        "        if add_bias:\n",
        "            x = np.column_stack([x, np.ones(Nt)])\n",
        "        yh = logistic(np.dot(x, self.w))  # predict output\n",
        "        return yh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1n2MaGay-E2"
      },
      "source": [
        "LogisticRegression.gradient = gradient # initialize the gradient method of the LogisticRegression class with gradient function\n",
        "\n",
        "taccs = {}\n",
        "vaccs = {}\n",
        "losses = {}\n",
        "for beta in [0.8, 0.9, 0.95, 0.99, 0]:\n",
        "    logres = LogisticRegression(max_iters=2e3, learning_rate=0.0002, momentum_beta=beta)\n",
        "    model, steps, train_accs, val_accs, best_val_acc, best_step, loss = logres.fit(train_x, train_y, val_x, val_y)\n",
        "    taccs[beta] = train_accs\n",
        "    vaccs[beta] = val_accs\n",
        "    losses[beta] = loss\n",
        "\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('accuracy')\n",
        "for beta, vals in taccs.items():\n",
        "    plt.plot(steps, vals, label=f'momentum coefficient {beta}')\n",
        "    plt.legend(loc='lower right', shadow=True, fontsize='medium')\n",
        "    plot.show()\n",
        "\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('accuracy')\n",
        "for beta, vals in vaccs.items():\n",
        "    plt.plot(steps, vals, label=f'momentum coefficient {beta}')\n",
        "    plt.legend(loc='lower right', shadow=True, fontsize='medium')\n",
        "    plot.show()\n",
        "\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('cross-entropy loss')\n",
        "for beta, vals in losses.items():\n",
        "    plt.plot(steps, vals, label=f'momentum coefficient {beta}')\n",
        "    plt.legend(loc='upper right', shadow=True, fontsize='medium')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUAGfckPjyr-"
      },
      "source": [
        "## Momemtum with Mini-Batch SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esd83ttgkjn_"
      },
      "source": [
        "logistic = lambda z: 1./ (1 + np.exp(-z))       #logistic function\n",
        "\n",
        "def gradient(self, x, y):\n",
        "    N,D = x.shape\n",
        "    yh = logistic(np.dot(x, self.w))    # predictions  size N\n",
        "    grad = np.dot(x.T, yh - y)/N        # divide by N because cost is mean over N points\n",
        "    return grad                         # size D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo5AtWCzGjk6"
      },
      "source": [
        "class MiniBatchLogisticRegression:\n",
        "\n",
        "    def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, batch_size=8, max_iters=1e5, verbose=False,\n",
        "                 eval_steps=100, shuffle=False, momentum_beta=0.9):\n",
        "        self.add_bias = add_bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon  # to get the tolerance for the norm of gradients\n",
        "        self.max_iters = max_iters  # maximum number of iteration of gradient descent\n",
        "        self.verbose = verbose\n",
        "        self.eval_steps = eval_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.momentum_beta = momentum_beta\n",
        "\n",
        "    def fit(self, x, y, val_x, val_y):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        if self.add_bias:\n",
        "            N = x.shape[0]\n",
        "            x = np.column_stack([x, np.ones(N)])\n",
        "        N, D = x.shape\n",
        "        self.w = np.zeros(D)\n",
        "        g = np.inf\n",
        "        t = 0\n",
        "        # the code snippet below is for gradient descent\n",
        "        steps = []\n",
        "        train_accs = []\n",
        "        val_accs = []\n",
        "        best_val_acc = 0\n",
        "        best_step = 0\n",
        "        prev = np.zeros(D)\n",
        "        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:\n",
        "            if self.shuffle:\n",
        "                p = np.random.permutation(int(y.shape[0]))\n",
        "                x = x[p]\n",
        "                y = y[p]\n",
        "            if self.batch_size != 600:\n",
        "                for i in range(0, int(y.shape[0]), self.batch_size):\n",
        "                    batch_x = x[i:i+self.batch_size]\n",
        "                    batch_y = y[i:i+self.batch_size]\n",
        "                    g = self.gradient(batch_x, batch_y)\n",
        "                    g = self.momentum_beta * prev + (1 - self.momentum_beta) * g\n",
        "                    self.w = self.w - self.learning_rate * g\n",
        "                    prev = g\n",
        "            else:\n",
        "                g = self.gradient(x, y)\n",
        "                self.w = self.w - self.learning_rate * g\n",
        "                prev = g\n",
        "            t += 1\n",
        "            if t % self.eval_steps == 0:\n",
        "                print(t)\n",
        "                yh = self.predict(x)\n",
        "                yh = yh > 0.5\n",
        "                preds = y == yh\n",
        "                train_acc = sum(preds) / len(preds)\n",
        "\n",
        "                val_yh = self.predict(val_x, add_bias=True)\n",
        "                val_yh = val_yh > 0.5\n",
        "                val_preds = val_y == val_yh\n",
        "                val_acc = sum(val_preds) / len(val_preds)\n",
        "\n",
        "                steps.append(t)\n",
        "                train_accs.append(train_acc)\n",
        "                val_accs.append(val_acc)\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    best_step = t\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f'terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g)}')\n",
        "            print(f'the weight found: {self.w}')\n",
        "        return self, steps, train_accs, val_accs, best_val_acc, best_step\n",
        "\n",
        "    def predict(self, x, add_bias=False):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        Nt = x.shape[0]\n",
        "        if add_bias:\n",
        "            x = np.column_stack([x, np.ones(Nt)])\n",
        "        yh = logistic(np.dot(x, self.w))  # predict output\n",
        "        return yh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3siccTO1GrD1"
      },
      "source": [
        "MiniBatchLogisticRegression.gradient = gradient  # initialize the gradient method of the LogisticRegression class with gradient function\n",
        "\n",
        "for bs in [8, 64]:\n",
        "    taccs = {}\n",
        "    vaccs = {}\n",
        "    for beta in [0.8, 0.9, 0.95, 0.99, 0]:\n",
        "        logres = MiniBatchLogisticRegression(max_iters=1e6, learning_rate=0.0002, momentum_beta=beta, batch_size=bs)\n",
        "        model, steps, train_accs, val_accs, best_val_acc, best_step = logres.fit(train_x, train_y, val_x, val_y)\n",
        "        taccs[beta] = train_accs\n",
        "        vaccs[beta] = val_accs\n",
        "\n",
        "    plt.xlabel('iterations')\n",
        "    plt.ylabel('accuracy')\n",
        "    for beta, vals in taccs.items():\n",
        "        plt.plot(steps, vals, label=f'momentum coefficient {beta}')\n",
        "        plt.legend(loc='lower right', shadow=True, fontsize='medium')\n",
        "        plt.show()\n",
        "\n",
        "    plt.xlabel('iterations')\n",
        "    plt.ylabel('accuracy')\n",
        "    for beta, vals in vaccs.items():\n",
        "        plt.plot(steps, vals, label=f'momentum coefficient {beta}')\n",
        "        plt.legend(loc='lower right', shadow=True, fontsize='medium')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}